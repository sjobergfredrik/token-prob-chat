{"ast":null,"code":"import axios from 'axios';\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {Array} history - Previous chat messages\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, history = [], maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // Convert history to chat format and clean up the tokens\n    const messages = [\n    // System message to set the context\n    {\n      role: 'system',\n      content: 'You are a helpful assistant. Always maintain context from the conversation history when answering questions. If a question seems to reference previous messages, consider that context in your response.'\n    },\n    // Convert history messages\n    ...history.map(msg => ({\n      role: msg.type === 'user' ? 'user' : 'assistant',\n      content: msg.type === 'user' ? msg.content : msg.tokens.map(t => t.token.replace(/^Ġ/g, ' ').replace(/^▁/g, ' ')).join('').trim()\n    }))];\n\n    // Add current prompt\n    messages.push({\n      role: 'user',\n      content: prompt\n    });\n\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(CHAT_URL, {\n      model: \"gpt-4o-mini\",\n      messages: messages,\n      max_tokens: maxTokens,\n      temperature,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    // For probabilities, we only use the current prompt to keep responses focused\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(COMPLETIONS_URL, {\n      model: \"gpt-3.5-turbo-instruct\",\n      prompt: formattedPrompt,\n      max_tokens: maxTokens,\n      temperature,\n      logprobs,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3,\n      echo: false\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    var _error$response;\n    console.error('Error calling OpenAI API:', ((_error$response = error.response) === null || _error$response === void 0 ? void 0 : _error$response.data) || error.message);\n    throw error;\n  }\n};","map":{"version":3,"names":["axios","API_KEY","process","env","REACT_APP_OPENAI_API_KEY","COMPLETIONS_URL","CHAT_URL","getCompletion","prompt","history","maxTokens","temperature","logprobs","messages","role","content","map","msg","type","tokens","t","token","replace","join","trim","push","chatResponse","post","model","max_tokens","top_p","frequency_penalty","presence_penalty","headers","formattedPrompt","startsWith","probResponse","echo","choices","text","data","message","error","_error$response","console","response"],"sources":["/Users/sjobergf/Documents/chatProbs/token-prob-chat/src/services/openaiService.js"],"sourcesContent":["import axios from 'axios';\n\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {Array} history - Previous chat messages\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, history = [], maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // Convert history to chat format and clean up the tokens\n    const messages = [\n      // System message to set the context\n      {\n        role: 'system',\n        content: 'You are a helpful assistant. Always maintain context from the conversation history when answering questions. If a question seems to reference previous messages, consider that context in your response.'\n      },\n      // Convert history messages\n      ...history.map(msg => ({\n        role: msg.type === 'user' ? 'user' : 'assistant',\n        content: msg.type === 'user' \n          ? msg.content \n          : msg.tokens\n              .map(t => t.token.replace(/^Ġ/g, ' ').replace(/^▁/g, ' '))\n              .join('')\n              .trim()\n      }))\n    ];\n\n    // Add current prompt\n    messages.push({\n      role: 'user',\n      content: prompt\n    });\n\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(\n      CHAT_URL,\n      {\n        model: \"gpt-4o-mini\",\n        messages: messages,\n        max_tokens: maxTokens,\n        temperature,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    // For probabilities, we only use the current prompt to keep responses focused\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(\n      COMPLETIONS_URL,\n      {\n        model: \"gpt-3.5-turbo-instruct\",\n        prompt: formattedPrompt,\n        max_tokens: maxTokens,\n        temperature,\n        logprobs,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3,\n        echo: false\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n    \n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    console.error('Error calling OpenAI API:', error.response?.data || error.message);\n    throw error;\n  }\n}; "],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,OAAO,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;AACpD,MAAMC,eAAe,GAAG,uCAAuC;AAC/D,MAAMC,QAAQ,GAAG,4CAA4C;;AAE7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,aAAa,GAAG,MAAAA,CAAOC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAEC,SAAS,GAAG,GAAG,EAAEC,WAAW,GAAG,GAAG,EAAEC,QAAQ,GAAG,CAAC,KAAK;EAC7G,IAAI;IACF;IACA,MAAMC,QAAQ,GAAG;IACf;IACA;MACEC,IAAI,EAAE,QAAQ;MACdC,OAAO,EAAE;IACX,CAAC;IACD;IACA,GAAGN,OAAO,CAACO,GAAG,CAACC,GAAG,KAAK;MACrBH,IAAI,EAAEG,GAAG,CAACC,IAAI,KAAK,MAAM,GAAG,MAAM,GAAG,WAAW;MAChDH,OAAO,EAAEE,GAAG,CAACC,IAAI,KAAK,MAAM,GACxBD,GAAG,CAACF,OAAO,GACXE,GAAG,CAACE,MAAM,CACPH,GAAG,CAACI,CAAC,IAAIA,CAAC,CAACC,KAAK,CAACC,OAAO,CAAC,KAAK,EAAE,GAAG,CAAC,CAACA,OAAO,CAAC,KAAK,EAAE,GAAG,CAAC,CAAC,CACzDC,IAAI,CAAC,EAAE,CAAC,CACRC,IAAI,CAAC;IACd,CAAC,CAAC,CAAC,CACJ;;IAED;IACAX,QAAQ,CAACY,IAAI,CAAC;MACZX,IAAI,EAAE,MAAM;MACZC,OAAO,EAAEP;IACX,CAAC,CAAC;;IAEF;IACA,MAAMkB,YAAY,GAAG,MAAM1B,KAAK,CAAC2B,IAAI,CACnCrB,QAAQ,EACR;MACEsB,KAAK,EAAE,aAAa;MACpBf,QAAQ,EAAEA,QAAQ;MAClBgB,UAAU,EAAEnB,SAAS;MACrBC,WAAW;MACXmB,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE;IACpB,CAAC,EACD;MACEC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAUhC,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA;IACA,MAAMiC,eAAe,GAAG1B,MAAM,CAAC2B,UAAU,CAAC,GAAG,CAAC,GAAG3B,MAAM,GAAG,GAAG,GAAGA,MAAM;IACtE,MAAM4B,YAAY,GAAG,MAAMpC,KAAK,CAAC2B,IAAI,CACnCtB,eAAe,EACf;MACEuB,KAAK,EAAE,wBAAwB;MAC/BpB,MAAM,EAAE0B,eAAe;MACvBL,UAAU,EAAEnB,SAAS;MACrBC,WAAW;MACXC,QAAQ;MACRkB,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE,GAAG;MACrBK,IAAI,EAAE;IACR,CAAC,EACD;MACEJ,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAUhC,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA,OAAO;MACLqC,OAAO,EAAE,CAAC;QACRC,IAAI,EAAEb,YAAY,CAACc,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAACG,OAAO,CAAC1B,OAAO;QAClDH,QAAQ,EAAEwB,YAAY,CAACI,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAAC1B;MACzC,CAAC;IACH,CAAC;EACH,CAAC,CAAC,OAAO8B,KAAK,EAAE;IAAA,IAAAC,eAAA;IACdC,OAAO,CAACF,KAAK,CAAC,2BAA2B,EAAE,EAAAC,eAAA,GAAAD,KAAK,CAACG,QAAQ,cAAAF,eAAA,uBAAdA,eAAA,CAAgBH,IAAI,KAAIE,KAAK,CAACD,OAAO,CAAC;IACjF,MAAMC,KAAK;EACb;AACF,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}