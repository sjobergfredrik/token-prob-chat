{"ast":null,"code":"import axios from 'axios';\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(CHAT_URL, {\n      model: \"gpt-4o-mini\",\n      messages: [{\n        role: \"user\",\n        content: prompt\n      }],\n      max_tokens: maxTokens,\n      temperature,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(COMPLETIONS_URL, {\n      model: \"gpt-3.5-turbo-instruct\",\n      prompt: formattedPrompt,\n      max_tokens: maxTokens,\n      temperature,\n      logprobs,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3,\n      echo: false\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    var _error$response;\n    console.error('Error calling OpenAI API:', ((_error$response = error.response) === null || _error$response === void 0 ? void 0 : _error$response.data) || error.message);\n    throw error;\n  }\n};","map":{"version":3,"names":["axios","API_KEY","process","env","REACT_APP_OPENAI_API_KEY","COMPLETIONS_URL","CHAT_URL","getCompletion","prompt","maxTokens","temperature","logprobs","chatResponse","post","model","messages","role","content","max_tokens","top_p","frequency_penalty","presence_penalty","headers","formattedPrompt","startsWith","probResponse","echo","choices","text","data","message","error","_error$response","console","response"],"sources":["/Users/sjobergf/Documents/chatProbs/token-prob-chat/src/services/openaiService.js"],"sourcesContent":["import axios from 'axios';\n\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(\n      CHAT_URL,\n      {\n        model: \"gpt-4o-mini\",\n        messages: [\n          {\n            role: \"user\",\n            content: prompt\n          }\n        ],\n        max_tokens: maxTokens,\n        temperature,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(\n      COMPLETIONS_URL,\n      {\n        model: \"gpt-3.5-turbo-instruct\",\n        prompt: formattedPrompt,\n        max_tokens: maxTokens,\n        temperature,\n        logprobs,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3,\n        echo: false\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n    \n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    console.error('Error calling OpenAI API:', error.response?.data || error.message);\n    throw error;\n  }\n}; "],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,OAAO,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;AACpD,MAAMC,eAAe,GAAG,uCAAuC;AAC/D,MAAMC,QAAQ,GAAG,4CAA4C;;AAE7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,aAAa,GAAG,MAAAA,CAAOC,MAAM,EAAEC,SAAS,GAAG,GAAG,EAAEC,WAAW,GAAG,GAAG,EAAEC,QAAQ,GAAG,CAAC,KAAK;EAC/F,IAAI;IACF;IACA,MAAMC,YAAY,GAAG,MAAMZ,KAAK,CAACa,IAAI,CACnCP,QAAQ,EACR;MACEQ,KAAK,EAAE,aAAa;MACpBC,QAAQ,EAAE,CACR;QACEC,IAAI,EAAE,MAAM;QACZC,OAAO,EAAET;MACX,CAAC,CACF;MACDU,UAAU,EAAET,SAAS;MACrBC,WAAW;MACXS,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE;IACpB,CAAC,EACD;MACEC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAUrB,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA,MAAMsB,eAAe,GAAGf,MAAM,CAACgB,UAAU,CAAC,GAAG,CAAC,GAAGhB,MAAM,GAAG,GAAG,GAAGA,MAAM;IACtE,MAAMiB,YAAY,GAAG,MAAMzB,KAAK,CAACa,IAAI,CACnCR,eAAe,EACf;MACES,KAAK,EAAE,wBAAwB;MAC/BN,MAAM,EAAEe,eAAe;MACvBL,UAAU,EAAET,SAAS;MACrBC,WAAW;MACXC,QAAQ;MACRQ,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE,GAAG;MACrBK,IAAI,EAAE;IACR,CAAC,EACD;MACEJ,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAUrB,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA,OAAO;MACL0B,OAAO,EAAE,CAAC;QACRC,IAAI,EAAEhB,YAAY,CAACiB,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAACG,OAAO,CAACb,OAAO;QAClDN,QAAQ,EAAEc,YAAY,CAACI,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAAChB;MACzC,CAAC;IACH,CAAC;EACH,CAAC,CAAC,OAAOoB,KAAK,EAAE;IAAA,IAAAC,eAAA;IACdC,OAAO,CAACF,KAAK,CAAC,2BAA2B,EAAE,EAAAC,eAAA,GAAAD,KAAK,CAACG,QAAQ,cAAAF,eAAA,uBAAdA,eAAA,CAAgBH,IAAI,KAAIE,KAAK,CAACD,OAAO,CAAC;IACjF,MAAMC,KAAK;EACb;AACF,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}