{"ast":null,"code":"import axios from 'axios';\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {Array} history - Previous chat messages\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, history = [], maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // Convert history to chat format\n    const messages = history.map(msg => ({\n      role: msg.type === 'user' ? 'user' : 'assistant',\n      content: msg.type === 'user' ? msg.content : msg.tokens.map(t => t.token.replace('Ġ', ' ')).join('')\n    }));\n\n    // Add current prompt\n    messages.push({\n      role: 'user',\n      content: prompt\n    });\n\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(CHAT_URL, {\n      model: \"gpt-4o-mini\",\n      messages: messages,\n      max_tokens: maxTokens,\n      temperature,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    // For probabilities, we only use the current prompt to keep responses focused\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(COMPLETIONS_URL, {\n      model: \"gpt-3.5-turbo-instruct\",\n      prompt: formattedPrompt,\n      max_tokens: maxTokens,\n      temperature,\n      logprobs,\n      top_p: 0.9,\n      frequency_penalty: 0.3,\n      presence_penalty: 0.3,\n      echo: false\n    }, {\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${API_KEY}`\n      }\n    });\n\n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    var _error$response;\n    console.error('Error calling OpenAI API:', ((_error$response = error.response) === null || _error$response === void 0 ? void 0 : _error$response.data) || error.message);\n    throw error;\n  }\n};","map":{"version":3,"names":["axios","API_KEY","process","env","REACT_APP_OPENAI_API_KEY","COMPLETIONS_URL","CHAT_URL","getCompletion","prompt","history","maxTokens","temperature","logprobs","messages","map","msg","role","type","content","tokens","t","token","replace","join","push","chatResponse","post","model","max_tokens","top_p","frequency_penalty","presence_penalty","headers","formattedPrompt","startsWith","probResponse","echo","choices","text","data","message","error","_error$response","console","response"],"sources":["/Users/sjobergf/Documents/chatProbs/token-prob-chat/src/services/openaiService.js"],"sourcesContent":["import axios from 'axios';\n\nconst API_KEY = process.env.REACT_APP_OPENAI_API_KEY;\nconst COMPLETIONS_URL = 'https://api.openai.com/v1/completions';\nconst CHAT_URL = 'https://api.openai.com/v1/chat/completions';\n\n/**\n * Gets both a high-quality response and token probabilities using a hybrid approach\n * @param {string} prompt - The user's input prompt\n * @param {Array} history - Previous chat messages\n * @param {number} maxTokens - Maximum number of tokens to generate\n * @param {number} temperature - Controls randomness (0-1)\n * @param {number} logprobs - Number of most likely tokens to return\n * @returns {Promise} - The API response\n */\nexport const getCompletion = async (prompt, history = [], maxTokens = 150, temperature = 0.3, logprobs = 5) => {\n  try {\n    // Convert history to chat format\n    const messages = history.map(msg => ({\n      role: msg.type === 'user' ? 'user' : 'assistant',\n      content: msg.type === 'user' ? msg.content : msg.tokens.map(t => t.token.replace('Ġ', ' ')).join('')\n    }));\n\n    // Add current prompt\n    messages.push({\n      role: 'user',\n      content: prompt\n    });\n\n    // First, get the high-quality response from gpt-4o-mini\n    const chatResponse = await axios.post(\n      CHAT_URL,\n      {\n        model: \"gpt-4o-mini\",\n        messages: messages,\n        max_tokens: maxTokens,\n        temperature,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n\n    // Then, get probability analysis using gpt-3.5-turbo-instruct\n    // For probabilities, we only use the current prompt to keep responses focused\n    const formattedPrompt = prompt.startsWith(' ') ? prompt : ' ' + prompt;\n    const probResponse = await axios.post(\n      COMPLETIONS_URL,\n      {\n        model: \"gpt-3.5-turbo-instruct\",\n        prompt: formattedPrompt,\n        max_tokens: maxTokens,\n        temperature,\n        logprobs,\n        top_p: 0.9,\n        frequency_penalty: 0.3,\n        presence_penalty: 0.3,\n        echo: false\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${API_KEY}`\n        }\n      }\n    );\n    \n    // Combine the responses - use the high-quality content but keep the probability data\n    return {\n      choices: [{\n        text: chatResponse.data.choices[0].message.content,\n        logprobs: probResponse.data.choices[0].logprobs\n      }]\n    };\n  } catch (error) {\n    console.error('Error calling OpenAI API:', error.response?.data || error.message);\n    throw error;\n  }\n}; "],"mappings":"AAAA,OAAOA,KAAK,MAAM,OAAO;AAEzB,MAAMC,OAAO,GAAGC,OAAO,CAACC,GAAG,CAACC,wBAAwB;AACpD,MAAMC,eAAe,GAAG,uCAAuC;AAC/D,MAAMC,QAAQ,GAAG,4CAA4C;;AAE7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,aAAa,GAAG,MAAAA,CAAOC,MAAM,EAAEC,OAAO,GAAG,EAAE,EAAEC,SAAS,GAAG,GAAG,EAAEC,WAAW,GAAG,GAAG,EAAEC,QAAQ,GAAG,CAAC,KAAK;EAC7G,IAAI;IACF;IACA,MAAMC,QAAQ,GAAGJ,OAAO,CAACK,GAAG,CAACC,GAAG,KAAK;MACnCC,IAAI,EAAED,GAAG,CAACE,IAAI,KAAK,MAAM,GAAG,MAAM,GAAG,WAAW;MAChDC,OAAO,EAAEH,GAAG,CAACE,IAAI,KAAK,MAAM,GAAGF,GAAG,CAACG,OAAO,GAAGH,GAAG,CAACI,MAAM,CAACL,GAAG,CAACM,CAAC,IAAIA,CAAC,CAACC,KAAK,CAACC,OAAO,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAACC,IAAI,CAAC,EAAE;IACrG,CAAC,CAAC,CAAC;;IAEH;IACAV,QAAQ,CAACW,IAAI,CAAC;MACZR,IAAI,EAAE,MAAM;MACZE,OAAO,EAAEV;IACX,CAAC,CAAC;;IAEF;IACA,MAAMiB,YAAY,GAAG,MAAMzB,KAAK,CAAC0B,IAAI,CACnCpB,QAAQ,EACR;MACEqB,KAAK,EAAE,aAAa;MACpBd,QAAQ,EAAEA,QAAQ;MAClBe,UAAU,EAAElB,SAAS;MACrBC,WAAW;MACXkB,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE;IACpB,CAAC,EACD;MACEC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAU/B,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA;IACA,MAAMgC,eAAe,GAAGzB,MAAM,CAAC0B,UAAU,CAAC,GAAG,CAAC,GAAG1B,MAAM,GAAG,GAAG,GAAGA,MAAM;IACtE,MAAM2B,YAAY,GAAG,MAAMnC,KAAK,CAAC0B,IAAI,CACnCrB,eAAe,EACf;MACEsB,KAAK,EAAE,wBAAwB;MAC/BnB,MAAM,EAAEyB,eAAe;MACvBL,UAAU,EAAElB,SAAS;MACrBC,WAAW;MACXC,QAAQ;MACRiB,KAAK,EAAE,GAAG;MACVC,iBAAiB,EAAE,GAAG;MACtBC,gBAAgB,EAAE,GAAG;MACrBK,IAAI,EAAE;IACR,CAAC,EACD;MACEJ,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAU/B,OAAO;MACpC;IACF,CACF,CAAC;;IAED;IACA,OAAO;MACLoC,OAAO,EAAE,CAAC;QACRC,IAAI,EAAEb,YAAY,CAACc,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAACG,OAAO,CAACtB,OAAO;QAClDN,QAAQ,EAAEuB,YAAY,CAACI,IAAI,CAACF,OAAO,CAAC,CAAC,CAAC,CAACzB;MACzC,CAAC;IACH,CAAC;EACH,CAAC,CAAC,OAAO6B,KAAK,EAAE;IAAA,IAAAC,eAAA;IACdC,OAAO,CAACF,KAAK,CAAC,2BAA2B,EAAE,EAAAC,eAAA,GAAAD,KAAK,CAACG,QAAQ,cAAAF,eAAA,uBAAdA,eAAA,CAAgBH,IAAI,KAAIE,KAAK,CAACD,OAAO,CAAC;IACjF,MAAMC,KAAK;EACb;AACF,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}